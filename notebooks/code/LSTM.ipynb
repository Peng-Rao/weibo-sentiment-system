{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:12:52.544355Z",
     "start_time": "2024-03-26T02:12:51.976694Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import load_corpus\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_PATH = \"../data/weibo_senti_100k/train.csv\"\n",
    "TEST_PATH = \"../data/weibo_senti_100k/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/4d/xybtwjyj2d3dv4p2_6s1yccw0000gn/T/jieba.cache\n",
      "Loading model cost 0.350 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "train_data = load_corpus(TRAIN_PATH)\n",
    "test_data = load_corpus(TEST_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:13:42.576536Z",
     "start_time": "2024-03-26T02:13:31.863912Z"
    }
   },
   "id": "84449db4fd4ca828",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  label\n0  乐乐 的 武汉 之 武汉 步行街 哈哈 我 窃以为 只 比 南京 的 夫子庙 好 了 那么 ...      1\n1           然后 去 医院 医生 给 火柴 包 起 一圈 绷带 出来 就 成 棉签 了 哈哈      1\n2                                 午饭 来不及 吃 先 垫下 肚子 泪      0\n3  汗 想想 办法 吧 狗 X 的 贵 G 福利 真 好 看 了 公司 照片 真是 本尊 啊 太强 了      0\n4                   昨天 被 人 恶作剧 了 表白 什么 的 不是 真的 晕 晕 晕      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>乐乐 的 武汉 之 武汉 步行街 哈哈 我 窃以为 只 比 南京 的 夫子庙 好 了 那么 ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>然后 去 医院 医生 给 火柴 包 起 一圈 绷带 出来 就 成 棉签 了 哈哈</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>午饭 来不及 吃 先 垫下 肚子 泪</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>汗 想想 办法 吧 狗 X 的 贵 G 福利 真 好 看 了 公司 照片 真是 本尊 啊 太强 了</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>昨天 被 人 恶作剧 了 表白 什么 的 不是 真的 晕 晕 晕</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据集\n",
    "df_train = pd.DataFrame(train_data, columns=[\"review\", \"label\"])\n",
    "df_test = pd.DataFrame(test_data, columns=[\"review\", \"label\"])\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:20.117788Z",
     "start_time": "2024-03-26T02:14:20.076437Z"
    }
   },
   "id": "1ad9f735eda9f1ca",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:22.542113Z",
     "start_time": "2024-03-26T02:14:22.537868Z"
    }
   },
   "id": "bf4f89dd1d1bd4dc",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0    [乐乐, 的, 武汉, 之, 武汉, 步行街, 哈哈, 我, 窃以为, 只, 比, 南京, ...\n1    [然后, 去, 医院, 医生, 给, 火柴, 包, 起, 一圈, 绷带, 出来, 就, 成,...\n2                           [午饭, 来不及, 吃, 先, 垫下, 肚子, 泪]\n3    [汗, 想想, 办法, 吧, 狗, X, 的, 贵, G, 福利, 真, 好, 看, 了, ...\n4       [昨天, 被, 人, 恶作剧, 了, 表白, 什么, 的, 不是, 真的, 晕, 晕, 晕]\nName: review, dtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec要求的输入格式: list(word)\n",
    "wv_input = df['review'].map(lambda s: s.split(\" \"))   # [for w in s.split(\" \") if w not in stopwords]\n",
    "wv_input.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:23.572715Z",
     "start_time": "2024-03-26T02:14:23.378780Z"
    }
   },
   "id": "f35f2b3abedea364",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raopend/Library/Caches/pypoetry/virtualenvs/weibo-sentiment-analysis-T4ra2412-py3.9/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 训练词向量\n",
    "from gensim import models\n",
    "# \n",
    "# word2vec = models.Word2Vec(wv_input, \n",
    "#                            vector_size=100,   # 词向量维度\n",
    "#                            min_count=1,      # 最小词频, 因为数据量较小, 这里卡1\n",
    "#                            workers=8,\n",
    "#                            epochs=100)      # 迭代轮次\n",
    "# word2vec.save(\"../models/word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:30.809010Z",
     "start_time": "2024-03-26T02:14:29.601947Z"
    }
   },
   "id": "2c7ab77e14978845",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('我', 0.8070616722106934),\n ('你们', 0.7611672878265381),\n ('他', 0.6921034455299377),\n ('她', 0.6857293844223022),\n ('他们', 0.6757404208183289),\n ('自己', 0.6481757760047913),\n ('我们', 0.6067619323730469),\n ('别人', 0.6038157939910889),\n ('的', 0.5902299880981445),\n ('妈妈', 0.5795382261276245)]"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看词向量训练效果\n",
    "# word2vec.wv.most_similar(\"你\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T15:42:29.887163Z",
     "start_time": "2024-03-25T15:42:29.795150Z"
    }
   },
   "id": "9e30a6720b3b4a83",
   "execution_count": 138
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('嘻嘻', 0.6881745457649231),\n ('哈哈哈', 0.6328049302101135),\n ('偷笑', 0.5760723948478699),\n ('哈哈哈哈', 0.5443381071090698),\n ('回复', 0.5043001174926758),\n ('媳妇', 0.4680759608745575),\n ('你', 0.44520074129104614),\n ('也', 0.43679869174957275),\n ('哈', 0.43084850907325745),\n ('啊', 0.41636788845062256)]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec.wv.most_similar(\"哈哈\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:35:00.562711Z",
     "start_time": "2024-03-25T14:35:00.495665Z"
    }
   },
   "id": "f07e987396c26d89",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('泪', 0.5092527866363525),\n ('失望', 0.4898751974105835),\n ('而终', 0.48545870184898376),\n ('悲伤', 0.4719094932079315),\n ('心碎', 0.46313390135765076),\n ('受伤', 0.4509361684322357),\n ('痛心', 0.45078450441360474),\n ('可怜', 0.4503190219402313),\n ('难兄难弟', 0.44628793001174927),\n ('心疼', 0.4447227716445923)]"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word2vec.wv.most_similar(\"伤心\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T14:35:03.063231Z",
     "start_time": "2024-03-25T14:35:03.013095Z"
    }
   },
   "id": "26bd1c31b7bc64f1",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# 加载词向量模型\n",
    "word2vec = models.Word2Vec.load(\"../models/word2vec.model\")\n",
    "\n",
    "# 自定义数据集\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, word2vec):\n",
    "        self.data = [self.process_sentence(s, word2vec) for s in df[\"review\"].tolist()]\n",
    "        self.label = df[\"label\"].to_numpy()\n",
    "\n",
    "    def process_sentence(self, sentence, word2vec):\n",
    "        vectors = [word2vec.wv[w] for w in sentence.split(\" \") if w in word2vec.wv.key_to_index]\n",
    "        return np.array(vectors)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return torch.tensor(data), torch.tensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    \n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    :param data: 第0维：data，第1维：label\n",
    "    :return: 序列化的data、记录实际长度的序列、以及label列表\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True) # pack_padded_sequence要求要按照序列的长度倒序排列\n",
    "    data_length = [len(sq[0]) for sq in data]\n",
    "    x = [i[0] for i in data]\n",
    "    y = [i[1] for i in data]\n",
    "    data = pad_sequence(x, batch_first=True, padding_value=0)   # 用RNN处理变长序列的必要操作\n",
    "    return data, torch.tensor(y, dtype=torch.float32), data_length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:41.471768Z",
     "start_time": "2024-03-26T02:14:39.893271Z"
    }
   },
   "id": "770e0572aace333f",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 网络结构\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)  # 双向, 输出维度要*2\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)  # 双向, 第一个维度要*2\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        packed_input = torch.nn.utils.rnn.pack_padded_sequence(input=x, lengths=lengths, batch_first=True)\n",
    "        packed_out, (h_n, h_c) = self.lstm(packed_input, (h0, c0))\n",
    "\n",
    "        lstm_out = torch.cat([h_n[-2], h_n[-1]], 1)  # 双向, 所以要将最后两维拼接, 得到的就是最后一个time step的输出\n",
    "        out = self.fc(lstm_out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:44.437096Z",
     "start_time": "2024-03-26T02:14:44.432230Z"
    }
   },
   "id": "70278f1e5f5955cc",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:14:50.920144Z",
     "start_time": "2024-03-26T02:14:50.892086Z"
    }
   },
   "id": "1fe0fa6ed6efc9b9",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 5e-4\n",
    "# 输入维度\n",
    "input_size = 768\n",
    "# 迭代轮次\n",
    "num_epochs = 100\n",
    "# 批次大小\n",
    "batch_size = 100\n",
    "# 词向量维度\n",
    "embed_size = 100\n",
    "# 隐藏层维度\n",
    "hidden_size = 64\n",
    "# LSTM层数\n",
    "num_layers = 2\n",
    "\n",
    "# 初始化模型\n",
    "model = LSTM(embed_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 加载数据集\n",
    "# 训练集\n",
    "train_data = TextDataset(df_train, word2vec)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "# 测试集\n",
    "test_data = TextDataset(df_test, word2vec)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:15:10.200616Z",
     "start_time": "2024-03-26T02:15:07.685544Z"
    }
   },
   "id": "de598e6cfadb6de2",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    # Loop through data loader data batches\n",
    "    for i, (X, labels, lengths)  in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X = X.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 1. Forward pass\n",
    "        outputs = model(X, lengths)\n",
    "        logits = outputs.view(-1)\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "        train_loss += loss.item()\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # 4. Loss backward\n",
    "        loss.backward(retain_graph=True)\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.round(outputs).squeeze()\n",
    "        train_acc += (y_pred_class == labels).sum().item() / len(labels)\n",
    "        \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss /= len(dataloader)\n",
    "    train_acc /= len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for i, (X, labels, lengths)  in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X = X.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # 1. Forward pass\n",
    "            outputs = model(X, lengths)\n",
    "            logits = outputs.view(-1)\n",
    "            # 2. Calculate  and accumulate loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "            # Calculate and accumulate accuracy\n",
    "            y_pred_class = torch.round(outputs).squeeze()\n",
    "            test_acc += (y_pred_class == labels).sum().item() / len(labels)\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss /= len(dataloader)\n",
    "    test_acc /= len(dataloader)\n",
    "    return test_loss, test_acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:15:10.944743Z",
     "start_time": "2024-03-26T02:15:10.937556Z"
    }
   },
   "id": "682470629d051d4c",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fef37989a7df4b7d93df740d26c928bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "results = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"test_loss\": [],\n",
    "    \"test_acc\": []\n",
    "}\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss, train_acc = train_step(model=model,\n",
    "                                       dataloader=train_loader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       optimizer=optimizer)\n",
    "    \n",
    "    test_loss, test_acc = test_step(model=model,\n",
    "                                    dataloader=test_loader,\n",
    "                                    loss_fn=loss_fn)\n",
    "    print(\n",
    "        f\"Epoch: {epoch+1} | \"\n",
    "        f\"train_loss: {train_loss:.4f} | \"\n",
    "        f\"train_acc: {train_acc:.4f} | \"\n",
    "        f\"test_loss: {test_loss:.4f} | \"\n",
    "        f\"test_acc: {test_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "    results[\"train_acc\"].append(train_acc)\n",
    "    results[\"test_loss\"].append(test_loss)\n",
    "    results[\"test_acc\"].append(test_acc)\n",
    "print(\"Finished training!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T02:23:09.247340Z",
     "start_time": "2024-03-26T02:16:29.714815Z"
    }
   },
   "id": "7629fec494ad9f5",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory \n",
    "MODEL_PATH = Path(\"../models/\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path \n",
    "MODEL_NAME = \"01_lstm_model.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "# 3. Save the model state dict \n",
    "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efbde9bfaf1a7649"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
